{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7WV6kzF3ohmrwEfNphlsg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhi-704/ERL/blob/master/QLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "NEED TO FIX CHOSEN ACTION AS CAN MAKE ILLEGAL MOVES\n",
        "'''"
      ],
      "metadata": {
        "id": "ZfzC9gZvz-Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo06dhI_yn2Z"
      },
      "outputs": [],
      "source": [
        "pip install jumanji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jumanji\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import sleep"
      ],
      "metadata": {
        "id": "BGbIW5UmzjGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for algorithm and agents generated\n",
        "epsilon = 0.01\n",
        "discount_factor = 0.99\n",
        "learning_rate = 0.20\n",
        "\n",
        "def get_action(state):\n",
        "  array = np.array(state.action_mask)\n",
        "  # Flatten the array\n",
        "  flattened_array = array.flatten()\n",
        "  # Get the indices of True values\n",
        "  true_indices = np.where(flattened_array)[0]\n",
        "  if len(true_indices) == 0:\n",
        "   return False\n",
        "  random_index = np.random.choice(true_indices)\n",
        "  # Convert the random index to two-dimensional coordinates\n",
        "  rotation, col_index = np.unravel_index(random_index, array.shape)\n",
        "  return [rotation,col_index]\n",
        "\n",
        "def get_valid_actions(state):\n",
        "  array = np.array(state.action_mask)\n",
        "  # Flatten the array\n",
        "  flattened_array = array.flatten()\n",
        "  # Get the indices of true values, which are the number of valid actions in the current state\n",
        "  true_indices = np.where(flattened_array)[0]\n",
        "\n",
        "  return true_indices\n",
        "\n",
        "def choose_action(Qvalues,state, state_tuple):\n",
        "  # # Makes sure the algorithm does not always pick the first index if all index values are the same\n",
        "  # if np.all(Qvalues[state] == Qvalues[state][0]):\n",
        "  #   return [(np.random.randint(0,3)), (np.random.randint(0,9))]\n",
        "  valid_actions = get_valid_actions(state)\n",
        "  if len(valid_actions) == 0:\n",
        "    return False\n",
        "  # Choose action with epsilon-greedy policy\n",
        "  if np.random.uniform(0,1) < epsilon:\n",
        "    action_index = np.random.choice(valid_actions, size = 1)\n",
        "  else:\n",
        "    # Access array associated with the state key and find the index of the maximum value\n",
        "    index_with_highest_Q_value = np.argmax(Qvalues[state_tuple][:, 1])\n",
        "    # Take only the valid action\n",
        "    action_index = Qvalues[state_tuple][index_with_highest_Q_value, 0]\n",
        "\n",
        "  # Convert the action index to two-dimensional coordinates\n",
        "  rotation, col_index = np.unravel_index(action_index, array.shape)\n",
        "\n",
        "  return [rotation, col_index], action_index\n",
        "\n",
        "\n",
        "def Qalgorithm(Qvalues):\n",
        "\n",
        "  # Initialises environment and rewards\n",
        "  rewards = []\n",
        "  # Reset your (jit-able) environment\n",
        "  key = jax.random.PRNGKey(0)\n",
        "  state, timestep = jax.jit(env.reset)(key)\n",
        "  state_tuple = tuple(state)\n",
        "\n",
        "  env.render(state)\n",
        "\n",
        "  valid_actions = get_valid_actions(state)\n",
        "\n",
        "  # Adds new state to Q values tables and model\n",
        "  if state_tuple not in Qvalues:\n",
        "    corresponding_array = [[x, 0] for x in valid_actions]\n",
        "    Qvalues.update({state_tuple: corresponding_array})\n",
        "\n",
        "  print(\"STATE UPDATED FIRST\")\n",
        "  print(Qvalues[state_tuple])\n",
        "\n",
        "  while True:\n",
        "\n",
        "    # Chooses max value action and takes the action\n",
        "    chosen_action, action_index = choose_action(Qvalues, state, state_tuple)\n",
        "    print(\"ACTION CHOSEN\")\n",
        "    # next_state, reward, terminal = env.step(chosen_action)\n",
        "    next_state, timestep = jax.jit(env.step)(state, chosen_action)\n",
        "    next_state_tuple = tuple(next_state)\n",
        "    curr_reward = next_state.reward\n",
        "\n",
        "    print(\"NEXT STATE UPDATED\")\n",
        "\n",
        "    new_valid_actions = get_valid_actions(next_state)\n",
        "    # Adds new state to Q values and Model\n",
        "    if next_state_tuple not in Qvalues:\n",
        "      corresponding_array2 = [[x,0] for x in new_valid_actions]\n",
        "      Qvalues.update({next_state_tuple: corresponding_array2})\n",
        "\n",
        "    print(\"NEXT STATE UPDATED SECOND\")\n",
        "    print(Qvalues[next_state_tuple])\n",
        "\n",
        "    # Adds reward to prior reward\n",
        "    rewards.append(curr_reward)\n",
        "\n",
        "    # Locates action index\n",
        "    chosen_action_index = np.where(Qvalues[state_tuple][:,0] == action_index)[0]\n",
        "\n",
        "    # Updates Q value table\n",
        "    Qvalues[state_tuple][chosen_action_index][1] += learning_rate * (curr_reward + discount_factor * np.max(Qvalues[next_state_tuple][:, 1]) - Qvalues[state_tuple][chosen_action_index][1])\n",
        "    print(\"Q VALUE UPDATED\")\n",
        "    print(Qvalues[state_tuple])\n",
        "    # Ends episode once terminal state is reached\n",
        "    if chosen_action is False:\n",
        "        break\n",
        "\n",
        "    print(\"TERMINAL PASSED\")\n",
        "    state = next_state\n",
        "    state_tuple = next_state_tuple\n",
        "\n",
        "    time.sleep(3)\n",
        "\n",
        "    env.render(state)\n",
        "\n",
        "\n",
        "  return sum(rewards)"
      ],
      "metadata": {
        "id": "A6VdI3aJzjpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_agents = 1\n",
        "\n",
        "def generateagent(episodes):\n",
        "  # Resets Q values and Model table\n",
        "  Qvalues = {}\n",
        "  # Generates an agent that follows QLearning for a number of episodes\n",
        "  total_rewards = np.zeros(episodes)\n",
        "  for i in range(len(total_rewards)):\n",
        "      total_rewards[i] = Qalgorithm(Qvalues)\n",
        "  return total_rewards"
      ],
      "metadata": {
        "id": "b0jWO9-lz3TB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}