{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMPZVI+lmK5i++aNXOsvxm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhi-704/ERL/blob/master/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj8yZbwW0Klm"
      },
      "outputs": [],
      "source": [
        "pip install jumanji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jumanji\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import sleep"
      ],
      "metadata": {
        "id": "-HAYvDfU0N2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main framework taken from https://github.com/ultronify/dqn-from-scratch-with-tf2/blob/master/\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.memory = deque(maxlen=1000)\n",
        "    self.capacity = 1000\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.memory)\n",
        "\n",
        "  def store(self, state, next_state, reward, action, done):\n",
        "    '''\n",
        "    Records a single step of game play experience\n",
        "    PARAM -\n",
        "    state: current game state\n",
        "    next_state: game state after taking action\n",
        "    reard: reward taking action at the current state brings\n",
        "    action: action taken at the current state\n",
        "    done: boolean to indicate if game is finished after taking action\n",
        "    RETURNS - N/A\n",
        "    '''\n",
        "    if len(self.memory) > self.capacity:\n",
        "      del self.memory[0]\n",
        "    self.memory.append((state, next_state, reward, action, done))\n",
        "\n",
        "\n",
        "  def sample(self):\n",
        "    '''\n",
        "    Samples a batch of gameplay experiences for training\n",
        "    PARAM - None\n",
        "    RETURNS - list of gameplay experiences\n",
        "    '''\n",
        "    batch_size = min(128, len(self.memory))\n",
        "    sample_batch = random.sample(self.memory, batch_size)\n",
        "    state_batch = []\n",
        "    next_state_batch = []\n",
        "    reward_batch = []\n",
        "    action_batch = []\n",
        "    done_batch = []\n",
        "    for experience in sample_batch:\n",
        "      state_batch.append(experience[0])\n",
        "      next_state_batch.append(experience[1])\n",
        "      reward_batch.append(experience[2])\n",
        "      action_batch.append(experience[3])\n",
        "      done_batch.append(experience[4])\n",
        "\n",
        "    return  np.array(state_batch), np.array(next_state_batch), np.array(reward_batch), np.array(action_batch), np.array(done_batch)"
      ],
      "metadata": {
        "id": "BGhqoRPx0Qx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DqnAgent:\n",
        "  '''\n",
        "  Create DQN agent class\n",
        "  '''\n",
        "\n",
        "  def __init__(self):\n",
        "    self.q_net = self.build_dqn_model()\n",
        "    self.target_q_net = self.build_dqn_model()\n",
        "\n",
        "  @staticmethod\n",
        "  def build_dqn_model():\n",
        "    '''\n",
        "    Builds deep neural network to predict Q values for all possible actions given a state.\n",
        "    Input should have shape of the state and the output should have the same shape as action space\n",
        "    RETURNS - Q network\n",
        "    '''\n",
        "    q_net = Sequential()\n",
        "    # Adds fully connected layer with 128 units and uses rectified linear unit activation function. he_uniform initliazes weight of layer\n",
        "    q_net.add(Dense(128, input_dim = 105, activation='relu', kernel_initializer='he_uniform'))\n",
        "    q_net.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
        "    # 40 actions so 40 different outputs\n",
        "    q_net.add(Dense(40, activation='linear', kernel_initializer='he_uniform'))\n",
        "\n",
        "    # opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "    q_net.compile(loss='mse', optimizer = 'adam')\n",
        "    return q_net\n",
        "\n",
        "  def convert_state(self,state):\n",
        "    '''\n",
        "    Convert state into observation variable that can be passed into neural network\n",
        "    PARAM -\n",
        "    state: current game state\n",
        "    RETURNS - observation variable\n",
        "    '''\n",
        "\n",
        "    grid = state.grid_padded.flatten().tolist()\n",
        "    tetromino = state.tetromino_index.flatten().tolist()\n",
        "    # print(grid)\n",
        "    # print(tetromino)\n",
        "    obs_variable = np.asarray(grid+tetromino)\n",
        "    # print(\"Current input dimensions:\")\n",
        "    # print(np.shape(obs_variable))\n",
        "    # print(obs_variable)\n",
        "\n",
        "    return obs_variable\n",
        "\n",
        "\n",
        "  def policy(self,state):\n",
        "    '''\n",
        "    Takes state from environment and returns an action that has the highest q value using epsilon_greedy\n",
        "    PARAM -\n",
        "    state: current game state\n",
        "    RETURNS - action\n",
        "    '''\n",
        "    # Matches state into 105, array\n",
        "    state_array = self.convert_state(state)\n",
        "    # Convert to into tensorflow tensor\n",
        "    state_input = tf.convert_to_tensor(state_array[None, :], dtype=tf.float32)\n",
        "    # Grabs Q values for all possible actions in current state\n",
        "    action_q = self.q_net(state_input)\n",
        "\n",
        "    action_mask = np.array(state.action_mask).flatten()\n",
        "\n",
        "    valid_q_values = action_q * action_mask\n",
        "    # Set elements that are zero to -999\n",
        "    valid_q_values = np.where(valid_q_values == 0, -99999999, valid_q_values)\n",
        "\n",
        "    # print(\"ACTION MASK\")\n",
        "    # print(action_mask)\n",
        "    # print(\"CORRESPONDING Q VALUES\")\n",
        "    # print(valid_q_values)\n",
        "    # print(np.shape(valid_q_values))\n",
        "    true_indices = self.get_valid_actions(state)\n",
        "    print(true_indices)\n",
        "    if len(true_indices) == 0:\n",
        "      return False, False\n",
        "    else:\n",
        "      action_index = np.argmax(valid_q_values, axis=1)[0]\n",
        "      # Convert action into acceptable type\n",
        "      array = np.array(state.action_mask)\n",
        "      rotation, col_index = np.unravel_index(action_index, array.shape)\n",
        "      action = [rotation, col_index]\n",
        "      return action, action_index\n",
        "\n",
        "  def get_valid_actions(self,state):\n",
        "    array = np.array(state.action_mask)\n",
        "    # Flatten the array\n",
        "    flattened_array = array.flatten()\n",
        "    # Get the indices of true values, which are the number of valid actions in the current state\n",
        "    true_indices = np.where(flattened_array)[0]\n",
        "\n",
        "    return true_indices\n",
        "\n",
        "  def random_action(self, state):\n",
        "    array = np.array(state.action_mask)\n",
        "    true_indices = self.get_valid_actions(state)\n",
        "    if len(true_indices) == 0:\n",
        "      return False\n",
        "    random_index = np.random.choice(true_indices)\n",
        "    # Convert the random index to two-dimensional coordinates\n",
        "    rotation, col_index = np.unravel_index(random_index, array.shape)\n",
        "    return [rotation,col_index]\n",
        "\n",
        "  def update_network(self):\n",
        "    '''\n",
        "    Updates current q network with q_net which brings all the training in q_net with target_q_net\n",
        "    '''\n",
        "    self.target_q_net.set_weights(self.q_net.get_weights())\n",
        "\n",
        "  def train(self,batch):\n",
        "    '''\n",
        "    Trains underlying network with batch of gameplay experineces to help it predict Q values\n",
        "    PARAM -\n",
        "    Batch: batch of experiences\n",
        "    RETURNS: Traning loss\n",
        "    '''\n",
        "    # Copying the batch over\n",
        "    state_batch, next_state_batch, reward_batch, action_batch, done_batch = batch\n",
        "\n",
        "    print(state_batch)\n",
        "    print(next_state_batch)\n",
        "    print(\"REWARD\")\n",
        "    print(reward_batch)\n",
        "    print(\"ACTION\")\n",
        "    print(action_batch)\n",
        "    print(done_batch)\n",
        "\n",
        "    # Running states through the q_net gives output Q values for the states\n",
        "    current_q = self.q_net(state_batch).numpy()\n",
        "    print(\"Printing current q\")\n",
        "    print(current_q)\n",
        "    # Copy over Q values for actions that weren't chosen\n",
        "    target_q = np.copy(current_q)\n",
        "    print(\"Printing target q\")\n",
        "    print(np.shape(target_q))\n",
        "    # Get the max Q values of states after transition by running next_state through target_q_net and take max Q values for all actions for each sample\n",
        "    next_q = self.target_q_net(next_state_batch).numpy()\n",
        "    max_next_q = np.amax(next_q, axis=1)\n",
        "    print(\"Entering loop\")\n",
        "    print(\"Printing next q\")\n",
        "    print(next_q)\n",
        "    print(\"Printing max next q\")\n",
        "    print(max_next_q)\n",
        "    # Update Q value of action taken with max Q value of next state plus intermediate reward from the action taken\n",
        "    for i in range(state_batch.shape[0]):\n",
        "      target_q_val = reward_batch[i].astype(float)\n",
        "      action_index = action_batch[i]\n",
        "      print(action_index)\n",
        "      if not done_batch[i]:\n",
        "        target_q_val += 0.95 * max_next_q[i]\n",
        "      target_q[i][action_index] = target_q_val\n",
        "    print(\"Finishing loop\")\n",
        "    # Train q_net with target Q values\n",
        "    training_his = self.q_net.fit(x = state_batch, y=target_q)\n",
        "    loss = training_his.history['loss']\n",
        "    print(\"Exiting train\")\n",
        "    return loss"
      ],
      "metadata": {
        "id": "q2BxFxyG0ZDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_training(env, agent):\n",
        "  '''\n",
        "  Evaluates performance of DQN agent and calculates average reward\n",
        "  PARAM -\n",
        "  env: game environment\n",
        "  agent: DQN agent\n",
        "  RETURNS: Average reward across episodes\n",
        "  '''\n",
        "  total_reward = 0.0\n",
        "  episodes_to_play = 6\n",
        "  for i in range(episodes_to_play):\n",
        "    key = jax.random.PRNGKey(1)\n",
        "    state, timestep = jax.jit(env.reset)(key)\n",
        "    done = False\n",
        "    episode_reward = 0.0\n",
        "    while not done:\n",
        "      action, action_index = agent.policy(state)\n",
        "      if action is False:\n",
        "        done = True\n",
        "        break\n",
        "      next_state, next_timestep = jax.jit(env.step)(state, action)\n",
        "      episode_reward += next_state.reward\n",
        "      state = next_state\n",
        "    total_reward += episode_reward\n",
        "  average_reward = total_reward / episodes_to_play\n",
        "  return average_reward\n",
        "\n",
        "def collect_experiences(env, agent, buffer):\n",
        "  '''\n",
        "  Collect gameplay experiences by playing with env and store experiences in buffer\n",
        "  '''\n",
        "  key = jax.random.PRNGKey(1)\n",
        "  state, timestep = jax.jit(env.reset)(key)\n",
        "  done = False\n",
        "  terminal = False\n",
        "\n",
        "\n",
        "  print(\"Entering Loop\")\n",
        "  while not done:\n",
        "    action, action_index = agent.policy(state)\n",
        "    print(action)\n",
        "    # Testing\n",
        "    #action = agent.random_action(state)\n",
        "    if action is False:\n",
        "      done = True\n",
        "      terminal = True\n",
        "      break\n",
        "    print(state.action_mask)\n",
        "    print(action)\n",
        "    next_state, next_timestep = jax.jit(env.step)(state, action)\n",
        "    print(\"Action taken\")\n",
        "    # env.render(state)\n",
        "    print(action)\n",
        "    buffer.store(agent.convert_state(state), agent.convert_state(next_state), next_state.reward, action_index, terminal)\n",
        "    state = next_state\n",
        "\n",
        "    # sleep(5)\n",
        "\n",
        "    env.render(state)\n",
        "\n",
        "def train_model(max_episodes = 10):\n",
        "  '''\n",
        "  Trains DQN agent to play game\n",
        "  RETURNS: None\n",
        "  '''\n",
        "\n",
        "  agent = DqnAgent()\n",
        "  buffer = ReplayBuffer()\n",
        "  # Instantiate tetris environment using registry\n",
        "  env = jumanji.make('Tetris-v0', num_rows = 5, time_limit = 1000)\n",
        "  # env = jumanji.make('Tetris-v0')\n",
        "\n",
        "\n",
        "  for episode_cnt in range(max_episodes):\n",
        "    collect_experiences(env, agent, buffer)\n",
        "    gameplay_batch = buffer.sample()\n",
        "    loss = agent.train(gameplay_batch)\n",
        "    print('So far the loss is {0}'.format(loss))\n",
        "    avg_reward = evaluate_training(env, agent)\n",
        "    print('So far the performance is {0}'.format(avg_reward))\n",
        "    # Update target q net every __ episodes (currently 2)\n",
        "    if episode_cnt % 2 == 0:\n",
        "      agent.update_network()\n",
        "#    sleep(5)"
      ],
      "metadata": {
        "id": "mzKZfzv80p0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model()\n",
        "print('No problems')"
      ],
      "metadata": {
        "id": "LttAL5lI0tdK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}